{
  "backend": "llama.cpp",
  "server_pool": {
    "size": 2,
    "host": "localhost",
    "port_start": 11601,
    "gpu_layers": 999,
    "request_timeout": 600,
    "health_timeout": 5.0
  },
  "gpu": {
    "enabled": true,
    "enable_gpu_monitoring": true,
    "allocation_strategy": "single-gpu-preferred",
    "gpu_allocation_strategy": "single-gpu-preferred",
    "monitoring_interval": 5.0,
    "cpu_fallback": true
  },
  "ollama": {
    "host": "localhost",
    "port": 11434,
    "timeout": 300.0
  },
  "server": {
    "host": "0.0.0.0",
    "port": 11555
  },
  "agents": []
}